"""
ä½¿ç”¨ LangGraph ä¸é˜¿é‡Œäº‘ç™¾ç‚¼çš„é€šä¹‰åƒé—®æ„å»ºèŠå¤©æœºå™¨äººã€‚

è¿è¡Œå‰è¯·ç¡®è®¤ï¼š
1. å·²å®‰è£… langgraphã€langchain-communityã€dashscope ç­‰ä¾èµ–ã€‚
2. åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® DASHSCOPE_API_KEY ä¸ºæœ‰æ•ˆçš„ API å¯†é’¥ã€‚
"""

import os
from typing import Annotated, Dict, Iterable
from typing_extensions import TypedDict
from dotenv import load_dotenv
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langchain_community.llms import Tongyi
from langchain_core.messages import AIMessage, HumanMessage

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

class ChatState(TypedDict):
    # ä¼šè¯çŠ¶æ€ä¿å­˜æ‰€æœ‰æ¶ˆæ¯ï¼›add_messages è¡¨ç¤ºæ¯æ¬¡æ›´æ–°æ—¶ä¼šè‡ªåŠ¨è¿½åŠ 
    messages: Annotated[list, add_messages]

def create_llm() -> Tongyi:
    """åˆå§‹åŒ–é€šä¹‰åƒé—®æ¨¡å‹åŒ…è£…å™¨ã€‚"""
    # ä»ç¯å¢ƒå˜é‡æˆ– .env æ–‡ä»¶è¯»å– API Key
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key or api_key == "your_dashscope_api_key":
        raise ValueError(
            "è¯·è®¾ç½® DASHSCOPE_API_KEYï¼\n"
            "æ–¹å¼1: åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : DASHSCOPE_API_KEY=sk-xxx\n"
            "æ–¹å¼2: è¿è¡Œå‰è®¾ç½®ç¯å¢ƒå˜é‡: export DASHSCOPE_API_KEY=sk-xxx"
        )
    return Tongyi(
        model="qwen-max",
        streaming=True,
        temperature=0.7,
        top_p=0.8,
        api_key=api_key,
    )

def chatbot_node(state: ChatState, llm: Tongyi) -> Dict[str, Iterable]:
    """
    ç”¨äº LangGraph çš„èŠ‚ç‚¹å‡½æ•°ã€‚
    è¯»å–å½“å‰çŠ¶æ€çš„æ¶ˆæ¯å†å²ï¼Œè°ƒç”¨æ¨¡å‹ç”Ÿæˆæ–°æ¶ˆæ¯ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ã€‚
    """
    # å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ä¾› LLM ä½¿ç”¨
    messages = state["messages"]
    
    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸º prompt
    # å¦‚æœéœ€è¦å¤šè½®å¯¹è¯ï¼Œå¯ä»¥æ„å»ºå®Œæ•´çš„å¯¹è¯å†å²
    user_messages = [msg for msg in messages if isinstance(msg, HumanMessage)]
    if not user_messages:
        return {"messages": [AIMessage(content="è¯·è¾“å…¥æ¶ˆæ¯ã€‚")]}
    
    # ä½¿ç”¨æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    prompt = user_messages[-1].content
    
    # éªŒè¯ prompt ä¸ä¸ºç©º
    if not prompt or not prompt.strip():
        return {"messages": [AIMessage(content="è¯·è¾“å…¥æœ‰æ•ˆçš„æ¶ˆæ¯ã€‚")]}
    
    # è°ƒç”¨ LLM è·å–å“åº”ï¼ˆè¿”å›å­—ç¬¦ä¸²ï¼‰
    response_text = llm.invoke(prompt)
    
    # å°†å“åº”åŒ…è£…æˆ AIMessage
    return {"messages": [AIMessage(content=response_text)]}

def build_graph(llm: Tongyi):
    """æ„å»º LangGraph çŠ¶æ€æœºã€‚è¿™é‡ŒåªåŒ…å«ä¸€ä¸ªèŠå¤©èŠ‚ç‚¹ã€‚"""
    graph_builder = StateGraph(ChatState)
    # ä½¿ç”¨ lambda å°† llm ç»‘å®šè¿›èŠ‚ç‚¹
    graph_builder.add_node("chatbot", lambda state: chatbot_node(state, llm))
    graph_builder.set_entry_point("chatbot")
    graph_builder.set_finish_point("chatbot")
    return graph_builder.compile()

def visualize_graph(graph) -> None:
    """å¯è§†åŒ–å›¾ç»“æ„å¹¶ä¿å­˜åˆ°æ–‡ä»¶ã€‚"""
    try:
        # ç”Ÿæˆ Mermaid å›¾
        mermaid_code = graph.get_graph().draw_mermaid()
        with open("chat_graph.mmd", "w") as f:
            f.write(mermaid_code)
        print("âœ… å›¾ç»“æ„å·²ä¿å­˜åˆ° chat_graph.mmd")
        
        # ç”Ÿæˆ PNG å›¾ç‰‡
        try:
            png_data = graph.get_graph().draw_mermaid_png()
            with open("chat_graph.png", "wb") as f:
                f.write(png_data)
            print("âœ… å›¾ç»“æ„å·²ä¿å­˜åˆ° chat_graph.png")
        except Exception as e:
            print(f"âš ï¸ PNG ç”Ÿæˆå¤±è´¥: {e}")
    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")

def interactive_chat(show_flow: bool = False) -> None:
    """å‘½ä»¤è¡Œä¸‹çš„å¯¹è¯å¾ªç¯ã€‚
    
    Args:
        show_flow: æ˜¯å¦æ˜¾ç¤ºæ¯ä¸€æ­¥çš„æ‰§è¡Œæµå‘
    """
    llm = create_llm()
    graph = build_graph(llm)
    
    # å¯è§†åŒ–å›¾ç»“æ„
    print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆå›¾ç»“æ„å¯è§†åŒ–...")
    visualize_graph(graph)
    
    print("\nèŠå¤©æœºå™¨äººå·²å¯åŠ¨ï¼Œè¾“å…¥æ¶ˆæ¯å¹¶æŒ‰å›è½¦å‘é€ã€‚")
    print("ç‰¹æ®Šå‘½ä»¤: exit/quit=é€€å‡º, flow=åˆ‡æ¢æµå‘æ˜¾ç¤º\n")
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ User: ")
        except EOFError:
            break
            
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºè¾“å…¥
        if not user_input.strip():
            continue
            
        if user_input.strip().lower() in {"exit", "quit"}:
            print("ğŸ¤– Assistant: Goodbye!")
            break
        
        # åˆ‡æ¢æµå‘æ˜¾ç¤º
        if user_input.strip().lower() == "flow":
            show_flow = not show_flow
            status = "å¼€å¯" if show_flow else "å…³é—­"
            print(f"âœ“ æµå‘è¿½è¸ªå·²{status}")
            continue
            
        # åˆå§‹çŠ¶æ€åŒ…å«ç”¨æˆ·æ¶ˆæ¯ï¼ˆä½¿ç”¨ HumanMessageï¼‰
        initial_state: ChatState = {
            "messages": [HumanMessage(content=user_input)]
        }
        
        # è°ƒç”¨ graph.stream é€æ­¥è·å–æ¨¡å‹è¾“å‡º
        if show_flow:
            print("\nğŸ”„ æ‰§è¡Œæµå‘:")
            
        for step_num, event in enumerate(graph.stream(initial_state), 1):
            if show_flow:
                node_name = list(event.keys())[0]
                print(f"  æ­¥éª¤ {step_num}: [{node_name}] æ­£åœ¨å¤„ç†...")
                
            for value in event.values():
                last_msg = value["messages"][-1]
                # æ£€æŸ¥æ˜¯å¦ä¸º AIMessage
                if isinstance(last_msg, AIMessage):
                    print(f"\nğŸ¤– Assistant: {last_msg.content}")
                    
    print("\nå¯¹è¯ç»“æŸã€‚")

if __name__ == "__main__":
    interactive_chat()
